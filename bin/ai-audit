#!/usr/bin/env python3
"""
PayReady AI Code Audit Tool
Date: September 18, 2025
Conducts comprehensive code audits focusing on quality, performance, and resilience
"""

import sys
import os
import json
import subprocess
from pathlib import Path
from typing import Dict, List, Tuple
from datetime import datetime

class CodeAuditor:
    def __init__(self):
        self.project_root = Path('/Users/lynnmusil/payready-ai')
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'files_analyzed': 0,
            'issues': [],
            'recommendations': [],
            'metrics': {}
        }

    def run_audit(self):
        """Execute comprehensive code audit."""
        print("ðŸ” PayReady AI Code Audit")
        print("=" * 60)
        print(f"ðŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        print(f"ðŸ“‚ Project: {self.project_root}")
        print()

        # 1. File Structure Analysis
        print("ðŸ“Š Analyzing project structure...")
        self.analyze_structure()

        # 2. Code Quality Check
        print("âœ¨ Checking code quality...")
        self.check_code_quality()

        # 3. Performance Analysis
        print("âš¡ Analyzing performance...")
        self.analyze_performance()

        # 4. Security & Resilience
        print("ðŸ”’ Checking security and resilience...")
        self.check_resilience()

        # 5. Documentation Coverage
        print("ðŸ“š Evaluating documentation...")
        self.check_documentation()

        # Generate report
        self.generate_report()

    def analyze_structure(self):
        """Analyze project file structure."""
        file_counts = {
            'python': 0,
            'bash': 0,
            'markdown': 0,
            'json': 0,
            'total': 0
        }

        for file_path in self.project_root.rglob('*'):
            if file_path.is_file() and not str(file_path).startswith('.'):
                file_counts['total'] += 1
                if file_path.suffix == '.py':
                    file_counts['python'] += 1
                elif file_path.suffix == '.sh' or 'bin/' in str(file_path):
                    file_counts['bash'] += 1
                elif file_path.suffix == '.md':
                    file_counts['markdown'] += 1
                elif file_path.suffix == '.json':
                    file_counts['json'] += 1

        self.results['metrics']['file_counts'] = file_counts
        self.results['files_analyzed'] = file_counts['total']

        # Check for missing important files
        important_files = [
            'README.md',
            '.gitignore',
            'requirements.txt',
            'setup.py'
        ]

        for filename in important_files:
            if not (self.project_root / filename).exists():
                self.results['issues'].append({
                    'type': 'structure',
                    'severity': 'medium',
                    'message': f'Missing {filename}'
                })

    def check_code_quality(self):
        """Check Python code quality."""
        python_files = list(self.project_root.rglob('*.py'))

        quality_issues = []
        for py_file in python_files:
            try:
                content = py_file.read_text()
                lines = content.split('\n')

                # Basic quality checks
                if not content.strip().startswith('"""') and not content.strip().startswith('#'):
                    quality_issues.append(f"{py_file.name}: Missing module docstring")

                # Check for long lines
                for i, line in enumerate(lines):
                    if len(line) > 120:
                        quality_issues.append(f"{py_file.name}:{i+1}: Line too long ({len(line)} chars)")

                # Check for TODO/FIXME comments
                for i, line in enumerate(lines):
                    if 'TODO' in line or 'FIXME' in line:
                        quality_issues.append(f"{py_file.name}:{i+1}: Found {line.strip()}")

            except Exception as e:
                quality_issues.append(f"{py_file.name}: Could not analyze - {e}")

        if quality_issues:
            self.results['issues'].extend([
                {'type': 'quality', 'severity': 'low', 'message': issue}
                for issue in quality_issues[:10]  # Limit to 10 issues
            ])

    def analyze_performance(self):
        """Analyze performance considerations."""
        perf_concerns = []

        # Check for potential performance issues in Python files
        for py_file in self.project_root.rglob('*.py'):
            try:
                content = py_file.read_text()

                # Check for common performance antipatterns
                if 'time.sleep' in content and 'test' not in py_file.name.lower():
                    perf_concerns.append(f"{py_file.name}: Uses time.sleep (blocking)")

                if content.count('for') > 10:
                    perf_concerns.append(f"{py_file.name}: Many loops - consider optimization")

                if 'subprocess.run' in content and 'timeout' not in content:
                    perf_concerns.append(f"{py_file.name}: subprocess.run without timeout")

            except:
                pass

        # Check bash scripts
        for sh_file in self.project_root.glob('bin/*'):
            if sh_file.is_file():
                try:
                    content = sh_file.read_text()
                    if 'curl' in content and 'timeout' not in content.lower():
                        perf_concerns.append(f"{sh_file.name}: curl without timeout")
                except:
                    pass

        if perf_concerns:
            self.results['issues'].extend([
                {'type': 'performance', 'severity': 'medium', 'message': concern}
                for concern in perf_concerns[:10]
            ])

    def check_resilience(self):
        """Check error handling and resilience."""
        resilience_issues = []

        # Check Python error handling
        for py_file in self.project_root.rglob('*.py'):
            try:
                content = py_file.read_text()

                # Check for bare except clauses
                if 'except:' in content:
                    resilience_issues.append(f"{py_file.name}: Bare except clause found")

                # Check for proper error handling in main functions
                if 'def main(' in content and 'try:' not in content:
                    resilience_issues.append(f"{py_file.name}: main() lacks error handling")

                # Check for API key exposure
                if 'API_KEY' in content and '=' in content and 'os.environ' not in content:
                    resilience_issues.append(f"{py_file.name}: Potential hardcoded API key")

            except:
                pass

        # Check bash scripts for error handling
        for sh_file in self.project_root.glob('bin/*'):
            if sh_file.is_file():
                try:
                    content = sh_file.read_text()
                    if '#!/' in content and 'set -e' not in content:
                        resilience_issues.append(f"{sh_file.name}: Missing 'set -e' for error handling")
                except:
                    pass

        if resilience_issues:
            self.results['issues'].extend([
                {'type': 'resilience', 'severity': 'high', 'message': issue}
                for issue in resilience_issues[:10]
            ])

    def check_documentation(self):
        """Check documentation coverage."""
        doc_files = list(self.project_root.glob('docs/**/*.md'))

        self.results['metrics']['documentation'] = {
            'files': len(doc_files),
            'categories': len(list(self.project_root.glob('docs/*/')))
        }

        if len(doc_files) < 5:
            self.results['issues'].append({
                'type': 'documentation',
                'severity': 'medium',
                'message': f'Only {len(doc_files)} documentation files found'
            })

    def generate_report(self):
        """Generate and display audit report."""
        print("\n" + "=" * 60)
        print("ðŸ“‹ AUDIT REPORT")
        print("=" * 60)

        # Summary
        print("\nðŸ“Š Summary:")
        print(f"  Files Analyzed: {self.results['files_analyzed']}")
        print(f"  Issues Found: {len(self.results['issues'])}")

        if 'file_counts' in self.results['metrics']:
            counts = self.results['metrics']['file_counts']
            print(f"  Python Files: {counts['python']}")
            print(f"  Bash Scripts: {counts['bash']}")
            print(f"  Documentation: {counts['markdown']}")

        # Issues by severity
        high_issues = [i for i in self.results['issues'] if i['severity'] == 'high']
        medium_issues = [i for i in self.results['issues'] if i['severity'] == 'medium']
        low_issues = [i for i in self.results['issues'] if i['severity'] == 'low']

        if high_issues:
            print("\nðŸ”´ High Priority Issues:")
            for issue in high_issues[:5]:
                print(f"  â€¢ {issue['message']}")

        if medium_issues:
            print("\nðŸŸ¡ Medium Priority Issues:")
            for issue in medium_issues[:5]:
                print(f"  â€¢ {issue['message']}")

        if low_issues:
            print("\nðŸŸ¢ Low Priority Issues:")
            for issue in low_issues[:3]:
                print(f"  â€¢ {issue['message']}")

        # Recommendations
        print("\nðŸ’¡ Recommendations:")
        recommendations = self.generate_recommendations()
        for rec in recommendations[:10]:
            print(f"  {rec}")

        # Save detailed report
        report_file = self.project_root / 'docs' / 'OPERATIONS' / 'AUDIT_REPORT.md'
        report_file.parent.mkdir(parents=True, exist_ok=True)

        report_content = self.format_markdown_report()
        report_file.write_text(report_content)

        print(f"\nðŸ“„ Detailed report saved to: {report_file}")

    def generate_recommendations(self) -> List[str]:
        """Generate recommendations based on audit findings."""
        recommendations = []

        # Based on issues found
        issue_types = set(i['type'] for i in self.results['issues'])

        if 'structure' in issue_types:
            recommendations.append("âœ… Add missing project files (README.md, requirements.txt)")

        if 'quality' in issue_types:
            recommendations.append("âœ… Add module docstrings to all Python files")
            recommendations.append("âœ… Configure code formatter (black) and linter (pylint)")

        if 'performance' in issue_types:
            recommendations.append("âš¡ Add timeouts to all external API calls")
            recommendations.append("âš¡ Consider async operations for I/O-heavy tasks")

        if 'resilience' in issue_types:
            recommendations.append("ðŸ”’ Replace bare except clauses with specific exceptions")
            recommendations.append("ðŸ”’ Add 'set -e' to all bash scripts for error handling")
            recommendations.append("ðŸ”’ Move all API keys to environment variables")

        if 'documentation' in issue_types:
            recommendations.append("ðŸ“š Expand documentation coverage")
            recommendations.append("ðŸ“š Add API documentation and examples")

        # General recommendations
        recommendations.extend([
            "ðŸŽ¯ Implement comprehensive unit tests",
            "ðŸŽ¯ Set up continuous integration (CI) pipeline",
            "ðŸŽ¯ Add type hints to Python functions",
            "ðŸŽ¯ Create development and deployment guides"
        ])

        return recommendations

    def format_markdown_report(self) -> str:
        """Format report as markdown."""
        report = f"""# PayReady AI Code Audit Report
**Generated**: {self.results['timestamp']}
**Status**: Complete

## Executive Summary
- **Files Analyzed**: {self.results['files_analyzed']}
- **Total Issues**: {len(self.results['issues'])}
- **High Priority**: {len([i for i in self.results['issues'] if i['severity'] == 'high'])}
- **Medium Priority**: {len([i for i in self.results['issues'] if i['severity'] == 'medium'])}
- **Low Priority**: {len([i for i in self.results['issues'] if i['severity'] == 'low'])}

## File Structure
"""
        if 'file_counts' in self.results['metrics']:
            for file_type, count in self.results['metrics']['file_counts'].items():
                report += f"- {file_type.capitalize()}: {count}\n"

        report += "\n## Issues Found\n\n"

        for severity in ['high', 'medium', 'low']:
            issues = [i for i in self.results['issues'] if i['severity'] == severity]
            if issues:
                report += f"### {severity.capitalize()} Priority\n"
                for issue in issues:
                    report += f"- [{issue['type']}] {issue['message']}\n"
                report += "\n"

        report += "## Recommendations\n"
        for rec in self.generate_recommendations():
            report += f"- {rec}\n"

        report += "\n---\n*This audit report is auto-generated. Review and prioritize based on your project needs.*"

        return report

def main():
    print("ðŸš€ Starting PayReady AI Code Audit...")
    auditor = CodeAuditor()
    auditor.run_audit()

if __name__ == "__main__":
    main()